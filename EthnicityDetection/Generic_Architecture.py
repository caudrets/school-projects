# -*- coding: utf-8 -*-
"""Ethnicity Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rvOaU3TKhndNfQqXLPdd7HlqA2JYEF9Z
"""

##################################
# Author: Christopher Audretsch
# Date: April 27, 2020
# Class: Image Processing, Adeel Bhutta
# This is a generic image classification network, based on https://colab.research.google.com/github/rpi-techfundamentals/fall2018-materials/blob/master/10-deep-learning/04-pytorch-mnist.ipynb
##################################

# imports
from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.autograd import Variable
from os import listdir
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import random

from google.colab import drive
drive.mount('/content/drive')

# Parameters

args={}
kwargs={}
args['batch_size']=512
args['test_batch_size']=512
args['epochs']=100  #The number of Epochs is the number of times you go through the full dataset. 
args['lr']=0.01 #Learning rate is how fast it will decend. 
args['momentum']=0.5 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).
args['seed']=1 #random seed
args['log_interval']=10
args['cuda']=True

path = "/content/drive/My Drive/Colab Notebooks/UTKFace"

class EthnicityDataset(Dataset):
    def __init__(self, root_dir, transform = None, train = True, post = False):
        # root_dir (string): Directory with all the images.
        # transform: 
        random.seed(42)
        self.root_dir = root_dir
        self.image_paths = listdir(root_dir)
        random.shuffle(self.image_paths)
        if post:
          self.image_paths = self.image_paths[12000:]
        elif train:
          self.image_paths = self.image_paths[:10000]
        else:
          self.image_paths = self.image_paths[10000:12000]
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open(self.root_dir + '/' + self.image_paths[idx])
        # The labels of each face image is embedded in the file name, formated like [age]_[gender]_[race]_[date&time].jpg
        # race is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others
        race = (self.image_paths[idx].split('_')[2])
        if race != '0' and race != '1' and race != '2' and race != '3' and race != '4':
          race = '4' ## we don't need to worry about this, only a few images in the dataset are messed up
        race = int(race)
        if self.transform:
          image = self.transform(image)
        return image, race

## Just a tool to calculate the mean of the dataset for normalization
## Not optimized and does not need to be ran. Values are hardcoded: mean=.497, stdv = .226

import numpy as np
import random
def compute_mean_and_std(grayscale=False, samples=2000):
    print('loading')
    dataset_train = EthnicityDataset(path)
    print('loaded')
    # Computationally intense, so only use a subset
    #del dataset_train.image_paths[:samples]
    to_pil = transforms.ToPILImage()
    to_grayscale = transforms.Grayscale(num_output_channels=3)
    to_tensor = transforms.ToTensor()

    r, g, b = [], [], []
    l = len(dataset_train)
    for i in range(1,samples) :
        x = random.randint(0,l - 1)
        img, race = dataset_train.__getitem__(x)
        if grayscale:
            img = to_grayscale(img)
        img = to_tensor(img).numpy()
        r.append(img[0]);
        g.append(img[1]);
        b.append(img[2])
    r, g, b = np.array(r).flatten(), np.array(g).flatten(), np.array(b).flatten()
    means = [color.mean() for color in (r, g, b)]
    stds = [color.std() for color in (r, g, b)]
    print(means, stds)
    return means, stds

## Creates our datasets with proper transformations
ds_train = EthnicityDataset(path,transform = transforms.Compose([
                       transforms.Grayscale(),
                       transforms.Resize(28),
                       transforms.ToTensor(),
                       transforms.Normalize((.497,), (.226,))
                   ]))
ds_test = EthnicityDataset(path, train = False, transform = transforms.Compose([
                       transforms.Grayscale(),
                       transforms.Resize(28),
                       transforms.ToTensor(),
                       transforms.Normalize((.497,), (.226,))
                   ]))

#load the data
train_loader = torch.utils.data.DataLoader(ds_train, batch_size=args['batch_size'], num_workers = 4, pin_memory = True, shuffle=True, **kwargs)
test_loader = torch.utils.data.DataLoader(ds_test,batch_size=args['test_batch_size'], num_workers = 4, pin_memory = True, shuffle=True, **kwargs)

## Generic Net. Two convolution layers, two fully connected layers.
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()  
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 5)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2)) 
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        print(x.size())
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        #Softmax gets probabilities. 
        return F.log_softmax(x, dim=1)

## Training and Testing functions

losses = [] ## loss values and accuracy values -- useful for graphing later
tests = []
def train(epoch):
    model.train()
    #print('before for loop')
    for batch_idx, (data, target) in enumerate(train_loader):
      #  print('in for loop')
        if args['cuda']:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        #This will zero out the gradients for this batch. 
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        #Print out the loss periodically. 
        if batch_idx % args['log_interval'] == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
    losses.append(loss.item())

def test():
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        if args['cuda']:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        x = pred.eq(target.data.view_as(pred)).long().cpu().sum()
        correct += x
    tests.append(int(correct) / len(test_loader.dataset))
    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

def test_post():
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in post_loader:
        if args['cuda']:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        x = pred.eq(target.data.view_as(pred)).long().cpu().sum()
        correct += x
    test_loss /= len(post_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(post_loader.dataset),
        100. * correct / len(post_loader.dataset)))

model = Net() # comment this out if you've already trained your net
if args['cuda']:
    model.cuda()

optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])

num_epochs = 5
for epoch in range(1, num_epochs):
    train(epoch)
    test()

## Here is a way to save and load the model

## Save:
# torch.save(model.state_dict(), '/content/drive/My Drive/Colab Notebooks/model_state.pt')
## Load:
# model = Net()
# model.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/model_state.pt'))
# model.eval()

# Test model out of sample
ds_post = EthnicityDataset(path,post = True, transform = transforms.Compose([
                       transforms.Grayscale(),
                       transforms.Resize(28),
                       transforms.ToTensor(),
                       transforms.Normalize((.497,), (.226,))
                   ]))
post_loader = torch.utils.data.DataLoader(ds_post, batch_size=512, num_workers = 4, pin_memory = True, shuffle=True, **kwargs)
test_post()

## Test model on one image
train_loader_one = torch.utils.data.DataLoader(ds_train, batch_size=1, num_workers = 4, pin_memory = True, shuffle=True, **kwargs)
for data, target in train_loader_one:
  data = data.cuda()
  target = target.cuda()
  data, target = Variable(data, volatile=True), Variable(target)
  output = model(data)
  pred = output.data.max(1, keepdim=True)[1]
  print(pred) ## our prediction
  print(pred.eq(target.data.view_as(pred)).long().cpu().sum()) ## did they match?
  print(target) ## the target
  break

## Test model on me and Adeel
## Not optimized
class Chris(Dataset):
    def __init__(self, transform = None, train = True):
        # root_dir (string): Directory with all the images.
        # transform: 
        self.image_paths = ['/content/chris2.jpg']
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open( self.image_paths[idx])
        print(image)
        # The labels of each face image is embedded in the file name, formated like [age]_[gender]_[race]_[date&time].jpg
        # race is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others
        race = 0
        if self.transform:
          image = self.transform(image)
        return image, race
class Adeel(Dataset):
    def __init__(self, transform = None, train = True):
        # root_dir (string): Directory with all the images.
        # transform: 
        self.image_paths = ['/content/adeel.jpg']
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open( self.image_paths[idx])
        print(image)
        # The labels of each face image is embedded in the file name, formated like [age]_[gender]_[race]_[date&time].jpg
        # race is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others
        race = 3
        if self.transform:
          image = self.transform(image)
        return image, race

c_ = Chris(transform = transforms.Compose([
                       transforms.Grayscale(),
                       transforms.Resize(28),
                       transforms.ToTensor(),
                       transforms.Normalize((.497,), (.226,))
]))
a = Adeel(transform = transforms.Compose([
                       transforms.Grayscale(),
                       transforms.Resize(28),
                       transforms.ToTensor(),
                       transforms.Normalize((.497,), (.226,))
]))
adeel_loader = torch.utils.data.DataLoader(a, batch_size=1, num_workers = 4, pin_memory = True, **kwargs)
chris_loader = torch.utils.data.DataLoader(c_, batch_size=1, num_workers = 4, pin_memory = True, **kwargs)
model.eval()
for data, target in chris_loader:
  #print(data)
  #print(target)
  data = data.cuda()
  target = target.cuda()
  data, target = Variable(data, volatile=True), Variable(target)
  output = model(data)
  pred = output.data.max(1, keepdim=True)[1]
  print('Prediction for Chris: ')
  print(pred[0][0]) ## our prediction
  print(pred.eq(target.data.view_as(pred)).long().cpu().sum()) ## did they match?
  print(target) ## the target
  break
for data, target in adeel_loader:
  #print(data)
  #print(target)
  data = data.cuda()
  target = target.cuda()
  data, target = Variable(data, volatile=True), Variable(target)
  output = model(data)
  pred = output.data.max(1, keepdim=True)[1]
  print('Prediction for Adeel: ')
  print(pred[0][0]) ## our prediction
  print(pred.eq(target.data.view_as(pred)).long().cpu().sum()) ## did they match?
  print(target) ## the target
  break

## Plot loss
import matplotlib.pyplot as plt
plt.plot(losses)
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.title('Loss Over Time')

## Plot accuracy
plt.plot(tests)
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.title('Model Accuracy Over Time')